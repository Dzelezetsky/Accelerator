{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "#import gym\n",
    "import argparse\n",
    "import os\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "#from sh_tools4maniskill import TD3, New_Trans_RB, env_constructor\n",
    "import copy\n",
    "#########################################################\n",
    "from collections import defaultdict\n",
    "\n",
    "import mani_skill.envs\n",
    "import gymnasium as gym\n",
    "from mani_skill.utils.wrappers.flatten import FlattenRGBDObservationWrapper\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "########################################################\n",
    "from torchvision.models import efficientnet_b0, mobilenet_v2\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.29.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import gymnasium\n",
    "gymnasium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import mujoco\n",
    "mujoco.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StateDictWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(StateDictWrapper, self).__init__(env)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)  # Получаем obs и info от reset()\n",
    "        # Преобразуем только obs в словарь с ключом 'state'\n",
    "        return {'state': obs}, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)  # Получаем obs, reward, done, info и т.д.\n",
    "        # Преобразуем только obs в словарь с ключом 'state'\n",
    "        return {'state': obs}, reward, terminated, truncated, info \n",
    "    \n",
    "\n",
    "def env_constructor(env_name, num_envs, obs_mode, reconf_freq=None):\n",
    "    if obs_mode == 'state':\n",
    "        env_kwargs = dict(obs_mode=obs_mode, sim_backend=\"gpu\", control_mode=\"pd_joint_delta_pos\")\n",
    "        env = gym.make(env_name, num_envs=num_envs, reconfiguration_freq=reconf_freq, **env_kwargs)\n",
    "        env = ManiSkillVectorEnv(env, num_envs, ignore_terminations=True, record_metrics=True)\n",
    "        env = StateDictWrapper(env)\n",
    "        s_d = env.observation_space.shape[-1]\n",
    "        a_d = env.action_space.shape[-1]\n",
    "        return env, s_d, a_d\n",
    "    elif obs_mode == 'rgb':\n",
    "        env_kwargs = dict(obs_mode=obs_mode, sim_backend=\"gpu\", control_mode=\"pd_joint_delta_pos\")\n",
    "        env = gym.make(env_name, num_envs=num_envs, reconfiguration_freq=reconf_freq, **env_kwargs)\n",
    "        env = FlattenRGBDObservationWrapper(env, rgb=True, depth=False, state=True)\n",
    "        env = ManiSkillVectorEnv(env, num_envs=num_envs, ignore_terminations=True, record_metrics=True)\n",
    "        s_d = env.observation_space['state'].shape[-1]\n",
    "        a_d = env.action_space.shape[-1]\n",
    "        return env, s_d, a_d\n",
    "    elif obs_mode == 'rgbd':\n",
    "        env_kwargs = dict(obs_mode=obs_mode, sim_backend=\"gpu\", control_mode=\"pd_joint_delta_pos\")\n",
    "        env = gym.make(env_name, num_envs=num_envs, reconfiguration_freq=reconf_freq, **env_kwargs)\n",
    "        env = FlattenRGBDObservationWrapper(env, rgb=True, depth=True, state=True)\n",
    "        env = ManiSkillVectorEnv(env, num_envs=num_envs, ignore_terminations=True, record_metrics=True)\n",
    "        s_d = env.observation_space['state'].shape[-1]\n",
    "        a_d = env.action_space.shape[-1]\n",
    "        return env, s_d, a_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Using the latest versioned environment `PushCube-v1` instead of the unversioned environment `PushCube`.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/random.py:183: UserWarning: CUDA reports that you have 2 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of CUDAs. If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using. For example, if you are using CPU only, set device.upper()_VISIBLE_DEVICES= or devices=[]; if you are using device 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env, s_d, a_d = env_constructor('PushCube', 3, 'rgb', reconf_freq=None)\n",
    "obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, d_model, state_dim, action_dim, max_action, obs_mode):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        in_channels = 3 if obs_mode=='rgb' else 4\n",
    "        \n",
    "        self.cnn = mobilenet_v2(pretrained=True)\n",
    "                                                                  \n",
    "        self.cnn_fc = nn.Linear(1000, d_model - d_model//4)\n",
    "\n",
    "        self.state_fc = nn.Linear(state_dim, d_model//4)\n",
    "\n",
    "        self.out_fc = nn.Linear(d_model, action_dim)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "\n",
    "    def forward(self, img, state):\n",
    "        if len( img.shape ) == 5: # ne, bs, h, w, c\n",
    "            ne, bs, h, w, c = img.shape[0], img.shape[1], img.shape[2], img.shape[3], img.shape[4]\n",
    "            img = torch.permute(img, (0, 1, 4, 2, 3))\n",
    "            img = img.reshape(ne*bs, c, h, w)\n",
    "        img = self.cnn(img/255.0)\n",
    "        img = img.reshape(ne, bs, 1000)\n",
    "        img = self.cnn_fc( img ) # ne, bs, d_model-d_model//4\n",
    "        \n",
    "        state = self.state_fc(state) # ne, bs, d_model//4\n",
    "        x = torch.cat([img, state],2) # ne, bs, d_model\n",
    "        \n",
    "        return self.max_action * torch.tanh(self.out_fc(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, d_model, state_dim, action_dim, obs_mode):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        in_channels = 3 if obs_mode=='rgb' else 4\n",
    "        self.cnn = mobilenet_v2(pretrained=True)                 \n",
    "        self.cnn_fc = nn.Linear(1000, d_model - d_model//4)\n",
    "        \n",
    "        self.state_fc = nn.Linear(state_dim, d_model//4)\n",
    "        \n",
    "        self.action_fc = nn.Linear(action_dim, d_model)\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.hidden_1 = nn.Linear(d_model*2, d_model)\n",
    "        self.hidden_2 = nn.Linear(d_model*2, d_model)\n",
    "        # Q2 architecture\n",
    "        self.out_fc_1 = nn.Linear(d_model, 1)\n",
    "        self.out_fc_2 = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, img, state, action):\n",
    "        if len( img.shape ) == 5: # ne, bs, h, w, c\n",
    "            ne, bs, h, w, c = img.shape[0], img.shape[1], img.shape[2], img.shape[3], img.shape[4]\n",
    "            img = torch.permute(img, (0, 1, 4, 2, 3))\n",
    "            img = img.reshape(ne*bs, c, h, w)\n",
    "        img = self.cnn(img/255.0)\n",
    "        img = img.reshape(ne, bs, 1000)\n",
    "        img = self.cnn_fc( img ) # ne, bs, d_model-d_model//4\n",
    "        \n",
    "        state = self.state_fc(state) # ne, bs, d_model//4\n",
    "        action = self.action_fc(action)\n",
    "        \n",
    "        sa = torch.cat([img, state, action],2) # ne, bs, d_model\n",
    "        \n",
    "        q1 = F.relu(self.hidden_1(sa))\n",
    "        q1 = self.out_fc_1(q1)\n",
    "        \n",
    "        q2 = F.relu(self.hidden_2(sa))\n",
    "        q2 = self.out_fc_2(q2)\n",
    "\n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "    def Q1(self, img, state, action):\n",
    "        \n",
    "        if len( img.shape ) == 5: # ne, bs, h, w, c\n",
    "            ne, bs, h, w, c = img.shape[0], img.shape[1], img.shape[2], img.shape[3], img.shape[4]\n",
    "            img = torch.permute(img, (0, 1, 4, 2, 3))\n",
    "            img = img.reshape(ne*bs, c, h, w)\n",
    "        img = self.cnn(img/255.0)\n",
    "        img = img.reshape(ne, bs, 1000)\n",
    "        img = self.cnn_fc( img ) # ne, bs, d_model-d_model//4\n",
    "        \n",
    "        state = self.state_fc(state) # ne, bs, d_model//4\n",
    "        action = self.action_fc(action)\n",
    "        \n",
    "        sa = torch.cat([img, state, action],2) # ne, bs, d_model\n",
    "        \n",
    "        q1 = F.relu(self.hidden_1(sa))\n",
    "        q1 = self.out_fc_1(q1)\n",
    "        \n",
    "        return q1\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dim_feedforward, dropout, wo_ffn, norm_first, use_gate, gate_mode, mode, layer_num=None):\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        self.use_gate = use_gate\n",
    "        self.wo_ffn = wo_ffn\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode == 'Trans':\n",
    "            self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        if not self.wo_ffn:\n",
    "            self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        if not self.wo_ffn:\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.dropout_ffn = nn.Dropout(dropout)\n",
    "            self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "            self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, src):                             # src = bs, seq_len, d_model\n",
    "        \n",
    "        skip_connection = src\n",
    "        \n",
    "        if self.norm_first:\n",
    "            src = self.layer_norm1(src)                 #bs, seq_len, d_model\n",
    "        \n",
    "        \n",
    "        if self.mode == 'Trans':\n",
    "            src2, _ = self.self_attn(src, src, src)\n",
    "\n",
    "        \n",
    "        connection = skip_connection + self.dropout1(src2)\n",
    "        \n",
    "        if not self.norm_first:\n",
    "            connection = self.layer_norm1(connection)\n",
    "\n",
    "        if self.wo_ffn:\n",
    "            return connection\n",
    "        ###########FFN PART##############\n",
    "        skip_connection2 = connection\n",
    "        if self.norm_first:\n",
    "            connection = self.layer_norm2(connection)\n",
    "        \n",
    "        src3 = self.linear2(self.dropout_ffn(self.relu(self.linear1(connection))))  #bs, seq_len, d_model\n",
    "        \n",
    "        if self.use_gate:\n",
    "            connection2, percentage2 = self.gate(skip_connection2, self.relu(src3))  # ВОЗМОЖНО ПОСЛЕ RELU НАДО ТОЖЕ ДОБАВИТЬ ДРОПАУТ\n",
    "        else: \n",
    "            connection2 = skip_connection2 + self.dropout2(src3)\n",
    "        \n",
    "        if not self.norm_first:\n",
    "            connection2 = self.layer_norm2(connection2)\n",
    "        \n",
    "        return connection2#, (percentage1, percentage2)\n",
    "    \n",
    "class Trans_Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, d_model=256, num_heads=2, num_layers=1, obs_mode='rgb'):\n",
    "        super(Trans_Critic, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        in_channels = 3 if obs_mode=='rgb' else 4\n",
    "        self.cnn = mobilenet_v2(pretrained=True)                 \n",
    "        self.cnn_fc = nn.Linear(1000, d_model - d_model//4)\n",
    "        self.state_fc = nn.Linear(state_dim, d_model//4)\n",
    "        self.action_fc = nn.Linear(action_dim, d_model//2)\n",
    "        self.transformer_encoder = CustomTransformerEncoder(d_model, num_heads, 512, 0.05, False, False, False, 'GRU', 'Trans')\n",
    "        # Q1 and Q2\n",
    "        self.out_fc_1 = nn.Linear(d_model+d_model//2, 1)\n",
    "        self.out_fc_2 = nn.Linear(d_model+d_model//2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, img, state, action):\n",
    "        n_e, bs, cont, s_d = state.shape\n",
    "        state = state.view(-1, cont, s_d)\n",
    "        img = img.view(-1, 3, 128, 128)\n",
    "        state = self.state_fc(state)  # n_e*bs, cont, d_model//4\n",
    "        img = self.cnn(img)            # n_e*bs*cont, 1000\n",
    "        img = self.cnn_fc(img)          # n_e*bs*cont,  d_model-d_model//4\n",
    "        img = img.view(-1, cont, self.d_model-self.d_model//4)  # n_e*bs, cont,  d_model-d_model//4\n",
    "        \n",
    "        x = torch.cat([img, state],-1)  # n_e*bs, cont, d_model\n",
    "        transformer_out = self.transformer_encoder(x)  # n_e*bs, cont, d_model\n",
    "        transformer_out = transformer_out[:, -1, :].view(n_e, bs, self.d_model)    # n_e, bs, d_model\n",
    "\n",
    "        action = self.action_fc(action)                     # n_e, bs, d_model//2\n",
    "        sa = torch.cat([transformer_out, action], dim=-1)   # n_e, bs, d_model+d_model//2\n",
    "        \n",
    "        q1 = self.out_fc_1(sa)\n",
    "        q2 = self.out_fc_2(sa)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, img, state, action):\n",
    "        n_e, bs, cont, s_d = state.shape\n",
    "        state = state.view(-1, cont, s_d)\n",
    "        img = img.view(-1, 3, 128, 128)\n",
    "        state = self.state_fc(state)  # n_e*bs, cont, d_model//4\n",
    "        img = self.cnn(img)            # n_e*bs*cont, 1000\n",
    "        img = self.cnn_fc(img)          # n_e*bs*cont,  d_model-d_model//4\n",
    "        img = img.view(-1, cont, self.d_model-self.d_model//4)  # n_e*bs, cont,  d_model-d_model//4\n",
    "        \n",
    "        x = torch.cat([img, state],-1)  # n_e*bs, cont, d_model\n",
    "        transformer_out = self.transformer_encoder(x)  # n_e*bs, cont, d_model\n",
    "        transformer_out = transformer_out[:, -1, :].view(n_e, bs, self.d_model)    # n_e, bs, d_model\n",
    "\n",
    "        action = self.action_fc(action)                     # n_e, bs, d_model//2\n",
    "        sa = torch.cat([transformer_out, action], dim=-1)   # n_e, bs, d_model+d_model//2\n",
    "        \n",
    "        q1 = self.out_fc_1(sa)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(5, 16, 5, 128, 128, 3).to(device)\n",
    "state = torch.randn(5, 16, 5, 25).to(device)\n",
    "actions = torch.randn(5, 16, 8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "tr = Trans_Critic(25, 8, d_model=512, num_heads=2, num_layers=1, obs_mode='rgb').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = tr.Q1(img, state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 16, 1]), torch.Size([5, 16, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape, q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = actor(img, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2 = critic(img, state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
